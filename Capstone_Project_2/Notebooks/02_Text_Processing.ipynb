{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "398d8324",
   "metadata": {},
   "source": [
    "1- Noise cleaning\n",
    "\n",
    "2- Contraction mapping\n",
    "\n",
    "3- Spell checking\n",
    "\n",
    "4- Stemming/Lemmatization\n",
    "\n",
    "5- Stopwords removal\n",
    "\n",
    "6- Case conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0f976ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "from textblob import TextBlob\n",
    "from contractions import CONTRACTION_MAP\n",
    "from nltk.tokenize.toktok import ToktokTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1527bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('Library/input_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3d9df7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57af21",
   "metadata": {},
   "source": [
    "### Noise Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b36564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing accented characters\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697a6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special characters\n",
    "def special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d62be",
   "metadata": {},
   "source": [
    "### Contraction Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4501dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contraction mapping\n",
    "def expand_contractions(text, contraction_mapping = CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expand_contraction = contraction_mapping.get(match)\\\n",
    "                            if contraction_mapping.get(match)\\\n",
    "                            else contraction_mapping.get(match.lower())\n",
    "        expand_contraction = first_char + expand_contraction[1:]\n",
    "        return expand_contraction\n",
    "    \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb0bf2",
   "metadata": {},
   "source": [
    "### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e743a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the mispelled words\n",
    "def correct_word_spelling(text):\n",
    "    text = TextBlob(text)\n",
    "    text = text.correct()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ab82f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/zori/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Removing meaningless words\n",
    "nltk.download('words')\n",
    "def remove_meaningless_words(text):\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    text = ' '.join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a39c0",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d4653d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68878d75",
   "metadata": {},
   "source": [
    "### Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a16dd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1ac5fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_text = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_text = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_text)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e387b5",
   "metadata": {},
   "source": [
    "### Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "502c60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_conversion(text, case = 'lower'):\n",
    "    if case == 'lower':\n",
    "        return text.lower()\n",
    "    elif case == 'upper':\n",
    "        return text.upper()\n",
    "    elif case == 'title':\n",
    "        return text.title()\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10dbaca",
   "metadata": {},
   "source": [
    "### Corpus Normalizer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f01cf8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_corpus(corpus, accented_chars=True, special_character=True, \n",
    "                    contraction_expansion=True, word_spelling=True, meaningless_word=True,\n",
    "                    lemmatization=True, stopwords=True, lower_case_conversion=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    for doc in corpus:\n",
    "        if accented_chars:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        if special_characters:\n",
    "            doc = special_characters(doc)\n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        if word_spelling:\n",
    "            doc = correct_word_spelling(doc)\n",
    "#         if meaningless_word:\n",
    "#             doc = remove_meaningless_words(doc)\n",
    "        if lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        if stopwords:\n",
    "            doc = remove_stopwords(doc)\n",
    "        if lower_case_conversion:\n",
    "            doc = text_conversion(doc)\n",
    "        normalized_corpus.append(doc)\n",
    "    \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1cf66630",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E866] Expected a string or 'Doc' as input, but got: <class 'textblob.blob.TextBlob'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6w/hzmbq90115906rg7xskdh8hm0000gn/T/ipykernel_42638/2767441685.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m normalized_corpus(df.comment_text, accented_chars=True, special_character=True, \n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mcontraction_expansion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_spelling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeaningless_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     lemmatization=True, stopwords=True, lower_case_conversion=True)\n",
      "\u001b[0;32m/var/folders/6w/hzmbq90115906rg7xskdh8hm0000gn/T/ipykernel_42638/2859549273.py\u001b[0m in \u001b[0;36mnormalized_corpus\u001b[0;34m(corpus, accented_chars, special_character, contraction_expansion, word_spelling, meaningless_word, lemmatization, stopwords, lower_case_conversion)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#             doc = remove_meaningless_words(doc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6w/hzmbq90115906rg7xskdh8hm0000gn/T/ipykernel_42638/1628493888.py\u001b[0m in \u001b[0;36mlemmatize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \"\"\"\n\u001b[0;32m-> 1002\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE866\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ensure_doc_with_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E866] Expected a string or 'Doc' as input, but got: <class 'textblob.blob.TextBlob'>."
     ]
    }
   ],
   "source": [
    "normalized_corpus(df.comment_text, accented_chars=True, special_character=True, \n",
    "                    contraction_expansion=True, word_spelling=True, meaningless_word=True,\n",
    "                    lemmatization=True, stopwords=True, lower_case_conversion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1af47fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation edits made username Hardcore Metallica Fan reverted ? ' vandalisms , closure GAs voted New York Dolls FAC. please ' remove template talk page since ' retired now.89.205.38.27\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(df.comment_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9f0577e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.comment_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17f859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
